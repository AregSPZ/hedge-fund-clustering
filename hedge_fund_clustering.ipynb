{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGpqPA3fIGGtfJ+9ZsJzyM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AregSP/hedge-fund-clustering/blob/main/hedge_fund_clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The dataset used in this notebook was accessed [here](https://www.kaggle.com/datasets/ironbatshashank/top-100-global-hedge-funds-2020)."
      ],
      "metadata": {
        "id": "YrpPnLrZ-tFJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ct6H0qTiyzz0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "hedge_funds = pd.read_csv('hedge_funds_list.csv', names = ['name', 'city', 'country', 'aum', 'aum_change', 'strategy']).drop(0)\n",
        "hedge_funds"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset consists entirely of strings, and also there's some missing values."
      ],
      "metadata": {
        "id": "FMsHn5UCe-Tt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(hedge_funds.dtypes)\n",
        "print(hedge_funds.isna().sum())\n",
        "hedge_funds.describe()"
      ],
      "metadata": {
        "id": "hVUtBKVc0oxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The majority of hedge funds are located in a couple of US cities and London"
      ],
      "metadata": {
        "id": "mJeESMDsfIbs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hedge_funds['city'].value_counts()"
      ],
      "metadata": {
        "id": "4huOHP0t42BV",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hedge_funds['strategy'].value_counts()"
      ],
      "metadata": {
        "id": "bO-8ZN_l5OQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hedge_funds['country'].value_counts()"
      ],
      "metadata": {
        "id": "xwhCmW0O5jO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Theres a slight correlation between AUM and AUM change rate, implying that huge firms are getting more huge"
      ],
      "metadata": {
        "id": "SND3XJ5-ff1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hedge_funds_copy = hedge_funds.copy()\n",
        "\n",
        "# convert the numeric features to numeric data format\n",
        "hedge_funds_copy['aum'] = hedge_funds_copy['aum'].str.replace(',', '').astype(int)\n",
        "hedge_funds_copy['aum_change'] = pd.to_numeric(hedge_funds_copy['aum_change'])\n",
        "\n",
        "\n",
        "num_df = hedge_funds_copy[['aum', 'aum_change']]\n",
        "num_df.corr()"
      ],
      "metadata": {
        "id": "nCjqDibRM8AL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Numerical columns are skewed and would benefit from some sort of transformation, especially AUM"
      ],
      "metadata": {
        "id": "1SpahFfXIlez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "sns.pairplot(num_df)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zhD8inP0I7bT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Determine the best transformation for numerical columns by plotting square root, cube root, log and yeo johnson transformations"
      ],
      "metadata": {
        "id": "yG89VBN8J2tH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "\n",
        "# We'll create a function to plot different transformations\n",
        "\n",
        "def plot_transformations(df, column):\n",
        "  fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
        "  axs = axs.flatten()\n",
        "\n",
        "  # Square root transformation\n",
        "  df[column + '_sqrt'] = np.sqrt(df[column])\n",
        "  sns.histplot(df[column + '_sqrt'], ax=axs[0], kde=True)\n",
        "  axs[0].set_title('Square Root Transformation')\n",
        "\n",
        "  # Cube root transformation\n",
        "  df[column + '_cbrt'] = np.cbrt(df[column])\n",
        "  sns.histplot(df[column + '_cbrt'], ax=axs[1], kde=True)\n",
        "  axs[1].set_title('Cube Root Transformation')\n",
        "\n",
        "  # Log transformation (add 1 to avoid log(0))\n",
        "  df[column + '_log'] = np.log(df[column] + 1)\n",
        "  sns.histplot(df[column + '_log'], ax=axs[2], kde=True)\n",
        "  axs[2].set_title('Log Transformation')\n",
        "\n",
        "  # Yeo-Johnson transformation\n",
        "  pt = PowerTransformer(method='yeo-johnson')\n",
        "  df[column + '_yeo'] = pt.fit_transform(df[[column]])\n",
        "  sns.histplot(df[column + '_yeo'], ax=axs[3], kde=True)\n",
        "  axs[3].set_title('Yeo-Johnson Transformation')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "# Apply the function to numerical columns\n",
        "plot_transformations(hedge_funds_copy, 'aum')\n",
        "plot_transformations(hedge_funds_copy, 'aum_change')\n",
        "\n",
        "# best transformations:\n",
        "# aum - yeo johnson\n",
        "# aum change - log"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Duzut2FEJa0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "ZRAs-ez85v9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "num_columns = ['yeo_aum', 'log_aum_change']\n",
        "cat_columns = ['city', 'country']\n",
        "\n",
        "class CustomTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "      pass\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "      X_copy = X.copy()\n",
        "\n",
        "      # convert the numerical features to numeric dtypes\n",
        "      X_copy['aum'] = X_copy['aum'].str.replace(',', '').astype(int)\n",
        "      X_copy['aum_change'] = pd.to_numeric(X_copy['aum_change'])\n",
        "\n",
        "      # apply transformations on numeric features\n",
        "      X_copy['yeo_aum'] = PowerTransformer(method='yeo-johnson').fit_transform(X_copy[['aum']])\n",
        "      X_copy['log_aum_change'] = np.log(X_copy['aum_change'] + 1)\n",
        "\n",
        "      # merge rare cities and countries into 1 category\n",
        "      X_copy['city'] = X_copy['city'].apply(lambda x: 'Other' if x not in ['New York', 'London', 'Boston', 'Los Angeles'] else x)\n",
        "      X_copy['country'] = X_copy['country'].apply(lambda x: 'Other' if x not in ['United States', 'United Kingdom'] else x)\n",
        "\n",
        "      # the 'strategy' column contains the multiple strategies applied by the hedge fund seperated by commas. based on it we can create a binary / one hot encoded feature for each individual strategy, which results dozens of new features.\n",
        "\n",
        "      # this dictionary is meant to eliminate the duplicates of the same strategy which may appear because of whitespaces when splitting the original column\n",
        "\n",
        "      strategy_mapping = {\n",
        "                  'Multi Strategy': 'Multi Strategy',\n",
        "                  'Managed Futures': 'Managed Futures',\n",
        "                  'Event Driven': 'Event Driven',\n",
        "                  'Long/Short': 'Long/Short',\n",
        "                  'Global Macro': 'Global Macro',\n",
        "                  'Convertibles': 'Convertibles',\n",
        "                  'Crypto': 'Crypto',\n",
        "                  'Equity': 'Equity',\n",
        "                  'Global Equity': 'Global Equity',\n",
        "                  'Private Equity': 'Private Equity',\n",
        "                  'Special Situations': 'Special Situations',\n",
        "                  'Fund of Funds': 'Fund of Funds',\n",
        "                  'Absolute Returns': 'Absolute Returns',\n",
        "                  'Credit': 'Credit',\n",
        "                  'Fixed Income': 'Fixed Income',\n",
        "                  'Consumer Products': 'Consumer Products',\n",
        "                  'Commodities': 'Commodities',\n",
        "                  'Real Estate': 'Real Estate',\n",
        "                  'Distressed Assets': 'Distressed Assets',\n",
        "                  'Emerging Markets': 'Emerging Markets',\n",
        "                  'Energy': 'Energy',\n",
        "                  'Arbitrage': 'Arbitrage',\n",
        "                  'Value': 'Value',\n",
        "                  'Derivatives': 'Derivatives',\n",
        "                  'FX': 'FX',\n",
        "                  'Asian Equity': 'Asian Equity',\n",
        "                  'Activist': 'Activist',\n",
        "                  'Insurance': 'Insurance',\n",
        "                  'CDO': 'CDO',\n",
        "                  'UCITS': 'UCITS',\n",
        "                  'European Equity': 'European Equity'\n",
        "              }\n",
        "\n",
        "\n",
        "      # Apply the mapping to standardize strategy names (remove whitespaces,lowercase variants, etc)\n",
        "      X_copy['strategy'] = X_copy['strategy'].apply(lambda x: ','.join([strategy_mapping.get(item.strip(), item.strip()) for item in x.split(',')]))\n",
        "\n",
        "      # Split the strategy column by commas\n",
        "      X_copy_split = X_copy['strategy'].str.split(',', expand=True)\n",
        "\n",
        "      # Stack the split columns and get unique values\n",
        "      unique_values = pd.Series(X_copy_split.values.ravel()).dropna().unique()\n",
        "\n",
        "      # Create binary columns for each unique value (manual one hot encoding)\n",
        "      for value in unique_values:\n",
        "          X_copy[f'strategy_{value}'] = X_copy['strategy'].apply(lambda x: 1 if str(value) in x.split(',') else 0)\n",
        "\n",
        "      X_copy.drop(['name', 'strategy', 'aum', 'aum_change'], axis=1, inplace=True) # those features will not be used further\n",
        "\n",
        "      return X_copy\n",
        "\n",
        "\n",
        "# the transformer for filling missing values, normalizing numerical features and one hot encoding categorical columns\n",
        "fill_scale_encode = ColumnTransformer([\n",
        "    ('num', make_pipeline(SimpleImputer(strategy='median'), StandardScaler()), num_columns),\n",
        "    ('cat', make_pipeline(SimpleImputer(strategy='most_frequent'), OneHotEncoder(drop='first')), cat_columns)\n",
        "], remainder='passthrough', verbose_feature_names_out=False)\n",
        "\n",
        "\n",
        "pipeline = make_pipeline(CustomTransformer(), fill_scale_encode)\n",
        "\n",
        "X = pipeline.fit_transform(hedge_funds)\n",
        "X_df = pd.DataFrame(X, columns=pipeline.named_steps['columntransformer'].get_feature_names_out())\n",
        "X_df"
      ],
      "metadata": {
        "id": "k_kMszxX5xZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dimensionality Reduction and Clustering: determining the best combination of the number of clusters, t-sne's perplexity hyperparameter and the silhouette score."
      ],
      "metadata": {
        "id": "j6eQ7e4UrWQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE # a great choice for dimensionality reduction considering the complexity of the data, the desire for 2D visualization, and the small size of the dataset, which makes this algorithm computationally accessible\n",
        "from sklearn.cluster import KMeans # a standard choice for clustering\n",
        "from sklearn.metrics import silhouette_score # the baseline evaluation metric\n",
        "\n",
        "scores = []\n",
        "cluster_values = []\n",
        "perplexity_values = []\n",
        "\n",
        "perplexity = np.arange(0.91, 1.05, 0.01) # grid search for t-sne\n",
        "\n",
        "# for each number of clusters, this loop performs dimensionality reduction on the processed dataset using t-sne with a different perplexity each time, performs clustering on the reduced dataset using k-means, and stores the resulting silhouette score. at the end the best combination of the number of clusters, perplexity and silhouette score is printed.\n",
        "run = False\n",
        "if run:\n",
        "  for i in range(8, 31): # the number of clusters considered. this is the range that should provide meaningful results\n",
        "    for p in perplexity:\n",
        "      tsne = TSNE(n_components=2, random_state=42, perplexity=p)\n",
        "      X_tsne = tsne.fit_transform(X)\n",
        "      kmeans = KMeans(n_clusters=i, random_state=42, n_init=10)\n",
        "      kmeans.fit(X_tsne)\n",
        "      score = silhouette_score(X_tsne, kmeans.labels_)\n",
        "      scores.append(score)\n",
        "      cluster_values.append(i)\n",
        "      perplexity_values.append(p)\n",
        "\n",
        "  best_score_index = np.argmax(scores)  # Get the index of the best score\n",
        "  best_score = scores[best_score_index]\n",
        "  best_k = cluster_values[best_score_index]  # Get the corresponding cluster value\n",
        "  best_perplexity = perplexity_values[best_score_index]  # Get the corresponding perplexity value\n",
        "\n",
        "  print(f\"The best number of clusters is {best_k} with a silhouette score of {best_score} and perplexity of {best_perplexity}\")\n",
        "\n",
        "# The best number of clusters is 23 with a silhouette score of\n",
        "# ~0.857 and perplexity of 1.04"
      ],
      "metadata": {
        "id": "Z78OCT_EDKs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize the Perplexity, Clusters and Silhouette Scores combinations"
      ],
      "metadata": {
        "id": "_183j43auJ-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "plot = False\n",
        "if plot:\n",
        "  fig = plt.figure(figsize=(10, 8))\n",
        "  ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "  ax.scatter(perplexity_values, cluster_values, scores, c=scores, cmap='viridis')\n",
        "\n",
        "  ax.set_xlabel('Perplexity')\n",
        "  ax.set_ylabel('Number of Clusters')\n",
        "  ax.set_zlabel('Silhouette Score')\n",
        "\n",
        "  ax.set_title('3D Plot of Perplexity, Clusters, and Silhouette Scores')\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "17UXbmQZHeBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize the clusters"
      ],
      "metadata": {
        "id": "k6dLLkgZxXZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform final dimensionality reduction and clustering with derived hyperparameters\n",
        "X_tsne = TSNE(n_components=2, random_state=42, perplexity=1.04).fit_transform(X)\n",
        "\n",
        "kmeans = KMeans(n_clusters=23, random_state=42, n_init=10)\n",
        "kmeans.fit(X_tsne)\n",
        "\n",
        "# Generate a color palette with distinct colors\n",
        "palette = sns.color_palette(\"hsv\", 23)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 10))\n",
        "\n",
        "# Assign a different marker for each cluster\n",
        "markers = ['o', 's', 'D', '^', 'v', '<', '>', 'p', '*', 'h', 'H', 'd', 'P', 'X', '8', '1', '2', '3', '4', '+']\n",
        "\n",
        "for i in range(23):\n",
        "    cluster_points = X_tsne[kmeans.labels_ == i]\n",
        "    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], c=[palette[i]], label=f'Cluster {i}', marker=markers[i % len(markers)], s=150, alpha=0.6)\n",
        "\n",
        "# Plot cluster centroids\n",
        "centroids = kmeans.cluster_centers_\n",
        "plt.scatter(centroids[:, 0], centroids[:, 1], c='black', s=200, alpha=0.75, marker='X', label='Centroids')\n",
        "\n",
        "# Adding titles and labels\n",
        "plt.title('t-SNE visualization with K-means clustering')\n",
        "plt.xlabel('Dimension 1')\n",
        "plt.ylabel('Dimension 2')\n",
        "plt.grid(True)\n",
        "\n",
        "# Adding a legend\n",
        "plt.legend(loc='best', bbox_to_anchor=(1.05, 1), borderaxespad=0.)\n",
        "\n",
        "# Show plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# the clusters are well defined and contain reasonable numbers of instances."
      ],
      "metadata": {
        "id": "pvAiNLFRHkPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intrepret the results"
      ],
      "metadata": {
        "id": "7CgqosaU0RJf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a DataFrame with clusters as instances."
      ],
      "metadata": {
        "id": "cJXQhuAm0b3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_info = []\n",
        "\n",
        "for i in range(23):\n",
        "  cluster_data = X_df[kmeans.labels_ == i]\n",
        "  avg_aum = cluster_data['yeo_aum'].mean()\n",
        "  avg_aum_change = cluster_data['log_aum_change'].mean()\n",
        "\n",
        "  # number of instances in a cluster\n",
        "  num_instances = len(cluster_data)\n",
        "\n",
        "  # Calculate the most common strategy within each cluster\n",
        "  strategy_columns = [col for col in cluster_data.columns if col.startswith('strategy')]\n",
        "  if strategy_columns:\n",
        "    most_common_strategy = cluster_data[strategy_columns].sum().idxmax().split('_')[-1]\n",
        "  else:\n",
        "    most_common_strategy = 'N/A'\n",
        "\n",
        "  # Determine the most common country within each cluster\n",
        "  country_columns = [col for col in cluster_data.columns if col.startswith('country')]\n",
        "  if country_columns:\n",
        "    most_common_country = cluster_data[country_columns].sum().idxmax().split('_')[-1]\n",
        "  else:\n",
        "    most_common_country = 'N/A'\n",
        "\n",
        "  cluster_info.append({\n",
        "      'num_instances': num_instances,\n",
        "      'avg_yeo_aum': avg_aum,\n",
        "      'avg_log_aum_change': avg_aum_change,\n",
        "      'most_common_country': most_common_country,\n",
        "      'most_common_strategy': most_common_strategy,\n",
        "  })\n",
        "\n",
        "cluster_df = pd.DataFrame(cluster_info)\n",
        "cluster_df.index.name = 'Cluster'\n",
        "\n",
        "cluster_df\n"
      ],
      "metadata": {
        "id": "w2sfFe7O0Tg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizations"
      ],
      "metadata": {
        "id": "x1N0TfNKHmGf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Average AUM and AUM_change across clusters"
      ],
      "metadata": {
        "id": "3uB3OU8RYQCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Plot avg_yeo_aum on the left y-axis\n",
        "color = 'tab:blue'\n",
        "ax1.set_xlabel('Cluster')\n",
        "ax1.set_ylabel('avg_yeo_aum', color=color)\n",
        "ax1.plot(cluster_df.index, cluster_df['avg_yeo_aum'], color=color)\n",
        "ax1.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "# Ensure all x-axis labels are shown\n",
        "ax1.set_xticks(cluster_df.index)\n",
        "ax1.set_xticklabels(cluster_df.index, rotation=45, ha='right')\n",
        "\n",
        "# Add grid to the primary y-axis\n",
        "ax1.grid(True, which='both', axis='both', linestyle='--', linewidth=0.5)\n",
        "\n",
        "# Create a second y-axis for avg_log_aum_change\n",
        "ax2 = ax1.twinx()  # Create a twin axis sharing the same x-axis\n",
        "\n",
        "color = 'tab:red'\n",
        "ax2.set_ylabel('avg_log_aum_change', color=color)\n",
        "ax2.plot(cluster_df.index, cluster_df['avg_log_aum_change'], color=color)\n",
        "ax2.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "# Add grid to the secondary y-axis\n",
        "ax2.grid(True, which='both', axis='y', linestyle='--', linewidth=0.5)\n",
        "\n",
        "plt.title('avg_yeo_aum and avg_log_aum_change by Cluster')\n",
        "fig.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "IE4BJm6kX3Qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AUM vs AUM change"
      ],
      "metadata": {
        "id": "hoDOldRWZgVu"
      }
    },
    {
      "source": [
        "cluster_df.plot(kind='scatter', x='avg_yeo_aum', y='avg_log_aum_change', s=32, alpha=.8)\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)"
      ],
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "id": "esDEn73pXMZy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most Common Country"
      ],
      "metadata": {
        "id": "C68BtVd_Zb0i"
      }
    },
    {
      "source": [
        "cluster_df.groupby('most_common_country').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "id": "PhVrEU8tXcFi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Strategy Distributions"
      ],
      "metadata": {
        "id": "B_IbqycoZ2pp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_df.groupby('most_common_strategy').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)"
      ],
      "metadata": {
        "id": "jeCv_5GnZ5IB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can gain valuable insights from these plots. In particular, Cluster 14 with the highest AUM growth consists of 4 comparably little hedge funds located in London, UK. 2 hedge funds out of them share the Convertibles strategy."
      ],
      "metadata": {
        "id": "koH_qiRrazxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_14 = hedge_funds[kmeans.labels_ == 14]\n",
        "cluster_14"
      ],
      "metadata": {
        "id": "ePO0pHHPakpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then there's Cluster 20, which consists of 2 huge American hedge funds which experience decay. Both apply Multi Strategy."
      ],
      "metadata": {
        "id": "vdBKZpPGcvNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_20 = hedge_funds[kmeans.labels_ == 20]\n",
        "cluster_20"
      ],
      "metadata": {
        "id": "N6sFWwDmclQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cluster 11 consists of 2 huge American hedge funds which are doing pretty well. Both apply Multi Strategy."
      ],
      "metadata": {
        "id": "WN9-ONnUeshB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_11 = hedge_funds[kmeans.labels_ == 11]\n",
        "cluster_11"
      ],
      "metadata": {
        "id": "Zi0jKIr1eYH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cluster 3 is the biggest, followed by Cluster 7. This can be explained by the fact that both are average in terms of metrics and there's nothing that makes them stand out. Global Macro is the most popular strategy in both clusters."
      ],
      "metadata": {
        "id": "sGowkUEcsFrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_3 = hedge_funds[kmeans.labels_ == 3]\n",
        "cluster_3"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gkWsztUXru1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Little can be said about the remaining clusters. It is reasonable to make an assumption that they simply were formed by the instances with similar performance and strategies, again nothing special."
      ],
      "metadata": {
        "id": "K2fRTH1nwrcY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We can also use the clusters to gain insights about the strategies. For example, the plots below illustrate that all the clusters where Equity and Long/Short are the most popular strategies are below the 25th percentile in terms of AUM growth, while the most prosperous clusters (above 75th percentile) apply Convertibles and Commodities."
      ],
      "metadata": {
        "id": "jXUdflCLnpIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decaying_clusters = cluster_df[cluster_df['avg_log_aum_change'] < cluster_df['avg_log_aum_change'].quantile(0.25)]\n",
        "\n",
        "prospering_clusters = cluster_df[cluster_df['avg_log_aum_change'] > cluster_df['avg_log_aum_change'].quantile(0.75)]\n",
        "\n",
        "\n",
        "(decaying_clusters.groupby('most_common_strategy').size() / cluster_df.groupby('most_common_strategy').size()).plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)\n",
        "plt.show()\n",
        "\n",
        "(prospering_clusters.groupby('most_common_strategy').size() / cluster_df.groupby('most_common_strategy').size()).plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ALsot8mgmJcp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}